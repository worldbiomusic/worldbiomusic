# 3 기계학습과 인식


# 3.1 기계 학습 기초


# 3.2 인공지능 제품의 설계와 구현
인공지능을 간단하게 분류하면 규칙기반, 고전적 기계학습, 딥러닝으로 나뉜다. 그중 딥러닝이 데이터를 준비하는 과정이 쉽다. 데이터의 레이블(실제 값)은 고전적 기계학습처럼 사람이 일일이 해줘야 하지만, 데이터의 특징 벡터를 알아내는 작업은 자동으로 이루어지기 때문이다 (자동으로 최적의 특징을 추출한다). 이를 특징학습(feature learning) 또는 표현 학습(representation learning)이라고 함.



# 3.3 데이터에 대한 이해


# 3.4 특징 추출과 표현
과정: 데이터 수집 -> 특징 추출 -> 모델링 -> 예측

보통 사람은 직관적으로 분별력이 높은 특징을 잘 추출해낸다. 이 처럼 인공지능에게 데이터를 구분하려할 때 분별력이 높은 특징을 사람이 정해주는것보다 딥러닝이 알아서 잘 찾는다. 

이러한 분류는 여러가지 특징들로 나뉜다. `수치형`, `범주형` 등. 그리고 이런 범주형같은 경우에는 one-hot code를 사용해서 구분하곤 한다 (이진수에 `0010`, `0001` 과 같이 구분하는 방법).


# 3.5 필기 숫자 인식


# 3.6 성능 측정
데이터의 특징에 따라 선택한 모델의 성능이 다를 수가 있다. 그래서 모델의 성능을 측정하는것도 중요. 그리고 테스트할 때 학습한 데이터를 가지고 테스트하는게 아니라 새로운 데이터로 테스트하는걸 `일반화`(generalization) 능력이라고 한다.

### 이진분류
![image](https://user-images.githubusercontent.com/61288262/226641997-88b8a27c-1e52-4b63-a35c-acc51e16cc28.png) (표가 가로 세로 축이 바뀐 사진이 많아서 조심해야 함)

<<P|N>: 뒤의 <P|N>는 모델이 예측한 결과를 positive | negative 로 나타냄, 앞의 true와 false는 뒤의 판단이 옮고 그름을 나타냄

- TP: true positive (참된 긍정: 참값을 제대로 맟춤)
- FP: false positive (거짓된 긍정: 거짓값을 긍정이라고 틀림)
- FN: false negative (거짓된 부정: 참값을 부정으로 틀림)
- TN: true negative (참된 부정: 거짓값을 제대로 맞춤)

### 파이썬 혼동행렬 정확도 구하는 법
![1](https://user-images.githubusercontent.com/61288262/226641799-c85d56f4-3156-416a-80d8-e5a4a7b51028.PNG)
먼저 `res`는 result, `conf` confused matrix의 줄임말이다. res는 x_test값으로 예측한 값이여서 예측한 y값 (변수로 만들어보자면 predict_y)으로 볼 수 있다. 그리고나서 conf행렬을 0으로 초기화해서 만들고 `conf[res[i]][y_test[i]]`으로 표를 만드는 작업을 한다 (2차원 배열의 특징을 이용해서 `res[i]`는 row를 증가시킨 값만큼, `y_test[i]`는 배열의 col을 증가시킨 값으로 매핑에서 x_test의 예측값(res)이 y_test(x_test의 정답)과 얼마나 정확도를 가지는지 혼동행렬을 만든 과정이다).


가장 널리 쓰이는 건 정확률(accuracy)이다 (맟힌 샘플 수 / 전체 샘플 수).

하지만 2부류에서는 한계가 있어서 특이도, 민감도, 정밀도 등이 사용됨


### 훈련, 검증, 테스트 집합 쪼개기
훈련으로 여러 모델을 훈련시켜보고 검증 데이터로 모델의 정확률을 비교해서 좋은 모델을 골라내고 테스트로 최종 모델의 정확률을 평가한다.


데이터를 샘플링해서 휸련, 테스트를 하다보면 우연히 높은 정확률을 가질 수도 아닐수도 있다. 그래서 k겹 교차검증(k-fold cross validation)을 사용해서 훈련집합을 k로 나누고 k-1개로 먼저 학습을 시킨다음에 마지막에 k번째 데이터로 평가를 한다. 이 작업을 나눈 부분 순서대로 k번 돌아가면서 정확률을 평균을 내서 얻는다. 이 방법으로 성능 측정 결과 신뢰도 높아짐.



# 3.7 인공지능은 어떻게 인식하나?
SVM같은 인식 알고리즘은 보통 `특징 공간 변환`과 `특징 공간 분할`로 분류 문제를 푼다. 모델들은 특징 공간을 변환해서 새로운 특징 공간으로 만든 뒤 선형 분류할 수 있게 만든다. 특징 공간에서 분할하는 경계를 결정 경계(decision boundary)라고 한다. 하지만 항상 데이터를 선형으로 분류할 수는 없다. 그러므로 SVM과 딥러닝같은 비선형 분류기(non-linear classifier)가 필요하다. 비선형으로 분류할 때는 과잉적합(overfitting)을 조심해야 한다.

**SVM 원리**

- 기계학습의 목적은 일반화 능력의 극대화
- 데이터 분류하는 경계는 데이터사이의 여백을 서로 최대화
- 사실 선형분류기이지만, kernel trick을 사용해서 공간을 비선형 공간으로 확장