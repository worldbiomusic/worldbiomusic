# 5.1 딥러닝의 등장
- `GPU, 많은 인터넷의 학습 데이터, 다양한 알고리즘`으로 딥러닝이 강해짐
- 파이썬 언어로 tensorflow와 keras가 주로 쓰임 (keras는 tensorflow를 한번더 추상화 함)



# 5.2 텐서플로 개념
- tensor: 차원을 의미함 (n차원 = n tensor)
- 2.0 부터 numpy와 호환됨
- 데이터의 구조(tensor)는 사용자가 맘대로 설정 가능 (사용할 때만 입력노드(1차원)에 잘 넣어주면 됨)


# 5.3 텐서플로 프로그래밍
- 텐서플로를 더 추상화한 라이브러리: keras (tensorflow에 편입됨)


# 5.4 텐서플로 다층 퍼셉트론
단순히 keras 소개


# 5.5 깊은 다층 퍼셉트론 (DMLP, Deep Multi-Layer Perceptron)
- DMLP는 다른말로 딥러닝
- 완전 연결 구조 (FC, Fully-Connected)
- 층 개수: 입력층 제외하고 은닉층 + 출력층 (예시. 입력층(1) + 은닉층(3) + 출력층(1) = 4층 (가중치도 4개))
- 출력층: 활성함수로 softmax 사용 (노드의 모든 출력은 1이므로 확률에 적합)



# 5.6 딥러닝의 학습 전략
**그레디언트 소멸 문제 (vanishing gradient)**  
- 은닉층이 여러개일 때, 오류 역전파를 할 때 뒤에서부터 업데이트하는데 점점 앞으로 갈수록 수정범위가 작아짐
```이유: 수정값은 오차에서 학습률(보통 0.001)만큼만 수정하는데 이는 바로 앞의 값을 가지고 업데이트하는것이므로, 먼저 끝 가중치는 출력값과 샘플의 레이블값의 차이의 "0.001"만큼 수정하고, 또 끝 에서 두번째 가중치는 끝 가중치의 오차의 "0.001"만큼 수정하므로 결과적으로 샘플의 레이블값과 출력값의 차이의 "0.001 * 0.001"만큼만 수정된 것이다. 그래서 입력층쪽의 가중치로 갈 수록 업데이트 수치가 상대적으로 너무 작아져서 학습에 시간이 너무 오래걸린다.```
- 해결법: 더 좋은 하드웨어(속도), 활성화함수 변경(ReLU)


**적합도**  
- 적합도: 학습데이터에 적합한 정도
- 일반화: 학습한 모델이 새로운 데이터에 대한 정확률 
- 적합도(fitness) 와 일반화(generalization)은 서로 상반관계 (trade-off)
- 과잉 적합(overfitting): 데이터에 비해 모델이 작은 경우 (학습이 너무 적게 됨) -> 모델 학습 더 하기
- 과소 적합(underfitting): 데이터에 비해 모델이 큰 경우 (학습이 너무 많이 됨) -> 데이터를 늘리기(data augmentation)
- 적합도 관련 규제(regularization): 데이터 양, 조기 멈춤, 가중치 감쇠, 드롭아웃, 앙상블


# 5.7 딥러닝의 손실함수
**손실함수 종류**  
- 절댓값: `|y-o|` (y: 샘플 레이블, o: 출력값) -> 별로(?)
- 평균제곱오차(MSE, Mean Squared Error): 통계학에서 많이 씀 -> 불공정함
- 교차 엔트로피(Cross Entropy): 두 확률분포의 불확실성 차이값 (평균제곱오차 불공정함 해결)


# 5.8 딥러닝의 옵티마이저
**최적화**는 신경망 학습, 즉 가중치(파라미터)를 학습 시키는 작업은 손실 함수의 최저점을 찾아가는 과정 (stochastic같은 옵티마이저(optimizer) 사용)

- 문제점: 데이터 잡음이 심해서 한계 -> 모멘텀, 적응적 학습률

**모멘텀(momentum)**  
원리: 경사하강할 때의 이전의 움직이던 관성을 이용

**적응적 학습률(adaptive learning rate)**  
원리: 학습률(p)를 조정하는 여러 기법
- Adagrad: 이전(과거) 그레디언트 기록된 정보 활용해 학습률 조정
- RMsprop: Adagrad에서 오래된 것의 영향을 줄여 향상됨
- Adam: RMsprop에서 모멘텀 원리적용해서 향상



# 5.9 좋은 프로그래밍 스킬
- 모듈화: 중복 제거
- 언어 특성 활용: 파이썬 굿굿
- 점증적 코딩: 차근차근 1개씩 늘려가
- 디자인 패턴: 이미 있는 좋은 패턴 학습
- 도구에 익숙: 언어특징들에 익숙
- 기초에 충실: 라이브러리 기초기능 또한 익숙



# 5.10 교차검증으로 하이퍼 매개변수 최적화
텐서플로는 교차 검증을 지원하지 않아서 sklearn 라이브러리의 `KFold` 클래스를 이용해서 사용

```python
def cross_validate(optimizer):
    accuracy = []
    for train_index, val_index in KFold(k).split(x_train):
        xtrain, xval = x_train[train_index], x_train[val_index]
        ytrain, yval = y_train[train_index], y_train[val_index]
        dmlp = build_model()
        dmlp.compile(loss=?, optimizer=optimizer, metrics=["accuracy"])
        dmlp.fit(xtrain, ytrain, batch_size=?, epochs=?, verbose=0)
        accracy.append(dmlp.evaluate(xval, yval, verbose=0)[1])
    return accuracy
```

